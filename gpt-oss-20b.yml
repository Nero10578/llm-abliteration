# GPT-OSS-20B Ablation Configuration Template
# This template is specifically configured for the GPT-OSS-20B model architecture

# Model identifier (local path or HuggingFace Hub ID)
model: "gpt-oss-20b-BF16/"

# Path to the measurement file generated by measure.py
measurements: "gpt-oss-20b-measurements.pt"

# Output directory for the ablated model
output: "gpt-oss-20b-ablated/"

# Ablation configurations
# Each entry specifies: layer, measurement, scale, sparsity
# - layer: Target layer index (0-23 for GPT-OSS-20B)
# - measurement: Which layer's refusal direction to use (can be different from target layer)
# - scale: Ablation strength (1.0 = full ablation, 0.0 = no ablation)
# - sparsity: Fraction of refusal direction to keep (1.0 = full, 0.5 = keep top 50% by magnitude)

ablate:
  # Example: Ablate layer 20 using its own refusal direction with full strength
  - layer: 20
    measurement: 20
    scale: 1.0
    sparsity: 1.0
  
  # Example: Ablate layer 21 using layer 20's refusal direction with moderate strength
  - layer: 21
    measurement: 20
    scale: 0.8
    sparsity: 0.9
  
  # Example: Ablate layer 22 using layer 20's refusal direction with high strength and sparsity
  - layer: 22
    measurement: 20
    scale: 1.2
    sparsity: 0.7
  
  # Example: Ablate layer 23 using layer 20's refusal direction with full strength
  - layer: 23
    measurement: 20
    scale: 1.0
    sparsity: 1.0

# Notes for GPT-OSS-20B:
# - The model has 24 layers (indices 0-23)
# - Uses MoE architecture with experts and router
# - Key parameters that will be ablated:
#   * self_attn.o_proj.weight (attention output projection)
#   * mlp.experts.*.down_proj (expert down-projection only)
#   * mlp.router.weight (expert routing weights only, not bias)
# - The "sinks" parameter in self_attn is unique to GPT-OSS and will be ignored